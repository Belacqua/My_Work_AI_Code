{"cells":[{"cell_type":"markdown","metadata":{"id":"BN-y_hn25ycn"},"source":["**Plan for Task AB-44**\n","\n","Core LLM-based Task Extraction Engine for AB-44\n","\n","\n","*Objectives*\n","\n","* Implement robust document processing with hierarchical awareness\n","\n","* Build advanced LLM-based task extraction with chain-of-thought reasoning\n","\n","* Create structured output schema for database integration\n","\n","* Develop initial unit tests for validation\n"]},{"cell_type":"markdown","metadata":{"id":"-3IFlT9wYFKO"},"source":["**Step 1: Library Installations**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EdBg5THRgk0G","colab":{"base_uri":"https://localhost:8080/","height":1000},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1743778791997,"user_tz":240,"elapsed":120632,"user":{"displayName":"David Scott","userId":"14102935354004324335"}},"outputId":"9faa4e03-2e04-4e3f-d787-b8990c07010a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting llama-index\n","  Downloading llama_index-0.12.28-py3-none-any.whl.metadata (12 kB)\n","Collecting llama-parse\n","  Downloading llama_parse-0.6.4.post1-py3-none-any.whl.metadata (6.9 kB)\n","Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.70.0)\n","Collecting psycopg[binary]\n","  Downloading psycopg-3.2.6-py3-none-any.whl.metadata (4.4 kB)\n","Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama-index)\n","  Downloading llama_index_agent_openai-0.4.6-py3-none-any.whl.metadata (727 bytes)\n","Collecting llama-index-cli<0.5.0,>=0.4.1 (from llama-index)\n","  Downloading llama_index_cli-0.4.1-py3-none-any.whl.metadata (1.5 kB)\n","Collecting llama-index-core<0.13.0,>=0.12.28 (from llama-index)\n","  Downloading llama_index_core-0.12.28-py3-none-any.whl.metadata (2.6 kB)\n","Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama-index)\n","  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n","Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n","  Downloading llama_index_indices_managed_llama_cloud-0.6.11-py3-none-any.whl.metadata (3.6 kB)\n","Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama-index)\n","  Downloading llama_index_llms_openai-0.3.30-py3-none-any.whl.metadata (3.3 kB)\n","Collecting llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 (from llama-index)\n","  Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl.metadata (726 bytes)\n","Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama-index)\n","  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n","Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama-index)\n","  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n","Collecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama-index)\n","  Downloading llama_index_readers_file-0.4.7-py3-none-any.whl.metadata (5.4 kB)\n","Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n","  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n","Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (3.9.1)\n","Collecting llama-cloud-services>=0.6.4 (from llama-parse)\n","  Downloading llama_cloud_services-0.6.9-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.11/dist-packages (from psycopg[binary]) (4.13.0)\n","Collecting psycopg-binary==3.2.6 (from psycopg[binary])\n","  Downloading psycopg_binary-3.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.8 kB)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.1)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n","Requirement already satisfied: click<9.0.0,>=8.1.7 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services>=0.6.4->llama-parse) (8.1.8)\n","Collecting llama-cloud<0.2.0,>=0.1.17 (from llama-cloud-services>=0.6.4->llama-parse)\n","  Downloading llama_cloud-0.1.17-py3-none-any.whl.metadata (902 bytes)\n","Requirement already satisfied: platformdirs<5.0.0,>=4.3.7 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services>=0.6.4->llama-parse) (4.3.7)\n","Collecting python-dotenv<2.0.0,>=1.0.1 (from llama-cloud-services>=0.6.4->llama-parse)\n","  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n","Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.28->llama-index) (6.0.2)\n","Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.28->llama-index) (2.0.40)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.28->llama-index) (3.11.15)\n","Collecting banks<3.0.0,>=2.0.0 (from llama-index-core<0.13.0,>=0.12.28->llama-index)\n","  Downloading banks-2.1.1-py3-none-any.whl.metadata (11 kB)\n","Collecting dataclasses-json (from llama-index-core<0.13.0,>=0.12.28->llama-index)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.28->llama-index) (1.2.18)\n","Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.28->llama-index)\n","  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n","Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.13.0,>=0.12.28->llama-index)\n","  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.28->llama-index) (2025.3.2)\n","Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.28->llama-index) (1.6.0)\n","Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.28->llama-index) (3.4.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.28->llama-index) (2.0.2)\n","Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.28->llama-index) (11.1.0)\n","Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.28->llama-index) (2.32.3)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.28->llama-index) (9.0.0)\n","Collecting tiktoken>=0.3.3 (from llama-index-core<0.13.0,>=0.12.28->llama-index)\n","  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Collecting typing-inspect>=0.8.0 (from llama-index-core<0.13.0,>=0.12.28->llama-index)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.28->llama-index) (1.17.2)\n","Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.13.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.2.2)\n","Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n","  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n","Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n","  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.0)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.28->llama-index) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.28->llama-index) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.28->llama-index) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.28->llama-index) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.28->llama-index) (6.3.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.28->llama-index) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.28->llama-index) (1.18.3)\n","Collecting griffe (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.28->llama-index)\n","  Downloading griffe-1.7.2-py3-none-any.whl.metadata (5.0 kB)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.28->llama-index) (3.1.6)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.28->llama-index) (3.4.1)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.28->llama-index) (2.3.0)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.28->llama-index) (3.1.1)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.28->llama-index)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13.0,>=0.12.28->llama-index)\n","  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.2)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.28->llama-index) (24.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (1.17.0)\n","Collecting colorama>=0.4 (from griffe->banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.28->llama-index)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.28->llama-index) (3.0.2)\n","Downloading llama_index-0.12.28-py3-none-any.whl (7.0 kB)\n","Downloading llama_parse-0.6.4.post1-py3-none-any.whl (4.9 kB)\n","Downloading psycopg_binary-3.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading llama_cloud_services-0.6.9-py3-none-any.whl (29 kB)\n","Downloading llama_index_agent_openai-0.4.6-py3-none-any.whl (13 kB)\n","Downloading llama_index_cli-0.4.1-py3-none-any.whl (28 kB)\n","Downloading llama_index_core-0.12.28-py3-none-any.whl (1.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n","Downloading llama_index_indices_managed_llama_cloud-0.6.11-py3-none-any.whl (14 kB)\n","Downloading llama_index_llms_openai-0.3.30-py3-none-any.whl (23 kB)\n","Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl (5.9 kB)\n","Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n","Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n","Downloading llama_index_readers_file-0.4.7-py3-none-any.whl (40 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n","Downloading psycopg-3.2.6-py3-none-any.whl (199 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading banks-2.1.1-py3-none-any.whl (28 kB)\n","Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n","Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n","Downloading llama_cloud-0.1.17-py3-none-any.whl (253 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.9/253.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n","Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n","Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Downloading griffe-1.7.2-py3-none-any.whl (129 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.2/129.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Installing collected packages: striprtf, filetype, dirtyjson, python-dotenv, pypdf, psycopg-binary, psycopg, mypy-extensions, marshmallow, colorama, typing-inspect, tiktoken, griffe, llama-cloud, dataclasses-json, banks, llama-index-core, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-readers-llama-parse, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n","Successfully installed banks-2.1.1 colorama-0.4.6 dataclasses-json-0.6.7 dirtyjson-1.0.8 filetype-1.2.0 griffe-1.7.2 llama-cloud-0.1.17 llama-cloud-services-0.6.9 llama-index-0.12.28 llama-index-agent-openai-0.4.6 llama-index-cli-0.4.1 llama-index-core-0.12.28 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.11 llama-index-llms-openai-0.3.30 llama-index-multi-modal-llms-openai-0.4.3 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.7 llama-index-readers-llama-parse-0.4.0 llama-parse-0.6.4.post1 marshmallow-3.26.1 mypy-extensions-1.0.0 psycopg-3.2.6 psycopg-binary-3.2.6 pypdf-5.4.0 python-dotenv-1.1.0 striprtf-0.0.26 tiktoken-0.9.0 typing-inspect-0.9.0\n","Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.22)\n","Collecting langchain-openai\n","  Downloading langchain_openai-0.3.12-py3-none-any.whl.metadata (2.3 kB)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.49 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.49)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\n","Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.22)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.1)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n","Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.70.0)\n","Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (9.0.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (24.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (4.13.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.9.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.0)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain) (3.0.0)\n","Downloading langchain_openai-0.3.12-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: langchain-openai\n","Successfully installed langchain-openai-0.3.12\n","Collecting instructlab\n","  Downloading instructlab-0.24.3-py3-none-any.whl.metadata (63 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting boto3>=1.35.96 (from instructlab)\n","  Downloading boto3-1.37.27-py3-none-any.whl.metadata (6.7 kB)\n","Requirement already satisfied: click<9.0.0,>=8.1.7 in /usr/local/lib/python3.11/dist-packages (from instructlab) (8.1.8)\n","Collecting click-didyoumean>=0.3.0 (from instructlab)\n","  Downloading click_didyoumean-0.3.1-py3-none-any.whl.metadata (3.9 kB)\n","Collecting datasets>=2.18.0 (from instructlab)\n","  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from instructlab) (3.18.0)\n","Collecting gguf>=0.6.0 (from instructlab)\n","  Downloading gguf-0.14.0-py3-none-any.whl.metadata (3.7 kB)\n","Requirement already satisfied: GitPython>=3.1.42 in /usr/local/lib/python3.11/dist-packages (from instructlab) (3.1.44)\n","Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from instructlab) (0.28.1)\n","Collecting instructlab-eval<0.6.0,>=0.5.1 (from instructlab)\n","  Downloading instructlab_eval-0.5.1-py3-none-any.whl.metadata (12 kB)\n","Collecting instructlab-quantize>=0.1.0 (from instructlab)\n","  Downloading instructlab_quantize-0.1.0-py3-none-any.whl.metadata (2.7 kB)\n","Collecting instructlab-schema>=0.4.2 (from instructlab)\n","  Downloading instructlab_schema-0.4.2-py3-none-any.whl.metadata (1.7 kB)\n","Collecting instructlab-sdg<0.8.0,>=0.7.0 (from instructlab)\n","  Downloading instructlab_sdg-0.7.3-py3-none-any.whl.metadata (8.4 kB)\n","Collecting instructlab-training<0.8.0,>=0.7.0 (from instructlab)\n","  Downloading instructlab_training-0.7.0-py3-none-any.whl.metadata (15 kB)\n","Collecting llama_cpp_python==0.3.6 (from llama_cpp_python[server]==0.3.6->instructlab)\n","  Downloading llama_cpp_python-0.3.6.tar.gz (66.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting numpy<2.0.0,>=1.26.4 (from instructlab)\n","  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: openai>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from instructlab) (1.70.0)\n","Requirement already satisfied: peft>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from instructlab) (0.14.0)\n","Requirement already satisfied: prompt-toolkit>=3.0.38 in /usr/local/lib/python3.11/dist-packages (from instructlab) (3.0.50)\n","Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from instructlab) (2.11.1)\n","Collecting pydantic_yaml>=1.2.0 (from instructlab)\n","  Downloading pydantic_yaml-1.4.0-py3-none-any.whl.metadata (6.6 kB)\n","Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from instructlab) (6.0.2)\n","Requirement already satisfied: rich>=13.3.1 in /usr/local/lib/python3.11/dist-packages (from instructlab) (13.9.4)\n","Collecting rouge-score>=0.1.2 (from instructlab)\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting ruamel.yaml>=0.17.0 (from instructlab)\n","  Downloading ruamel.yaml-0.18.10-py3-none-any.whl.metadata (23 kB)\n","Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from instructlab) (0.2.0)\n","Requirement already satisfied: tokenizers>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from instructlab) (0.21.1)\n","Requirement already satisfied: toml>=0.10.2 in /usr/local/lib/python3.11/dist-packages (from instructlab) (0.10.2)\n","Requirement already satisfied: torch<2.7.0,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from instructlab) (2.6.0+cu124)\n","Requirement already satisfied: tqdm>=4.66.2 in /usr/local/lib/python3.11/dist-packages (from instructlab) (4.67.1)\n","Requirement already satisfied: transformers>=4.41.2 in /usr/local/lib/python3.11/dist-packages (from instructlab) (4.50.3)\n","Collecting trl<0.15.0,>=0.12.2 (from instructlab)\n","  Downloading trl-0.14.0-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: wandb>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from instructlab) (0.19.8)\n","Collecting xdg-base-dirs>=6.0.1 (from instructlab)\n","  Downloading xdg_base_dirs-6.0.2-py3-none-any.whl.metadata (3.6 kB)\n","Collecting psutil>=6.0.0 (from instructlab)\n","  Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n","Requirement already satisfied: huggingface_hub>=0.1.8 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_transfer]>=0.1.8->instructlab) (0.30.1)\n","Collecting haystack-ai>=2.8 (from instructlab)\n","  Downloading haystack_ai-2.12.0-py3-none-any.whl.metadata (14 kB)\n","Collecting docling-core>=2.10.0 (from docling-core[chunking]>=2.10.0->instructlab)\n","  Downloading docling_core-2.25.0-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: sentence-transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from instructlab) (3.4.1)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama_cpp_python==0.3.6->llama_cpp_python[server]==0.3.6->instructlab) (4.13.0)\n","Collecting diskcache>=5.6.1 (from llama_cpp_python==0.3.6->llama_cpp_python[server]==0.3.6->instructlab)\n","  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama_cpp_python==0.3.6->llama_cpp_python[server]==0.3.6->instructlab) (3.1.6)\n","Collecting uvicorn>=0.22.0 (from llama_cpp_python[server]==0.3.6->instructlab)\n","  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n","Collecting fastapi>=0.100.0 (from llama_cpp_python[server]==0.3.6->instructlab)\n","  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n","Collecting pydantic-settings>=2.0.1 (from llama_cpp_python[server]==0.3.6->instructlab)\n","  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n","Collecting sse-starlette>=1.6.1 (from llama_cpp_python[server]==0.3.6->instructlab)\n","  Downloading sse_starlette-2.2.1-py3-none-any.whl.metadata (7.8 kB)\n","Collecting starlette-context<0.4,>=0.3.6 (from llama_cpp_python[server]==0.3.6->instructlab)\n","  Downloading starlette_context-0.3.6-py3-none-any.whl.metadata (4.3 kB)\n","Collecting botocore<1.38.0,>=1.37.27 (from boto3>=1.35.96->instructlab)\n","  Downloading botocore-1.37.27-py3-none-any.whl.metadata (5.7 kB)\n","Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.35.96->instructlab)\n","  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n","Collecting s3transfer<0.12.0,>=0.11.0 (from boto3>=1.35.96->instructlab)\n","  Downloading s3transfer-0.11.4-py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.18.0->instructlab) (18.1.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.18.0->instructlab)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.18.0->instructlab) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.18.0->instructlab) (2.32.3)\n","Collecting xxhash (from datasets>=2.18.0->instructlab)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets>=2.18.0->instructlab)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.18.0->instructlab)\n","  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.18.0->instructlab) (3.11.15)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets>=2.18.0->instructlab) (24.2)\n","Collecting jsonref<2.0.0,>=1.1.0 (from docling-core>=2.10.0->docling-core[chunking]>=2.10.0->instructlab)\n","  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: jsonschema<5.0.0,>=4.16.0 in /usr/local/lib/python3.11/dist-packages (from docling-core>=2.10.0->docling-core[chunking]>=2.10.0->instructlab) (4.23.0)\n","Collecting latex2mathml<4.0.0,>=3.77.0 (from docling-core>=2.10.0->docling-core[chunking]>=2.10.0->instructlab)\n","  Downloading latex2mathml-3.77.0-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: pillow<12.0.0,>=10.0.0 in /usr/local/lib/python3.11/dist-packages (from docling-core>=2.10.0->docling-core[chunking]>=2.10.0->instructlab) (11.1.0)\n","Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from docling-core>=2.10.0->docling-core[chunking]>=2.10.0->instructlab) (0.9.0)\n","Collecting typer<0.13.0,>=0.12.5 (from docling-core>=2.10.0->docling-core[chunking]>=2.10.0->instructlab)\n","  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n","Collecting semchunk<3.0.0,>=2.2.0 (from docling-core[chunking]>=2.10.0->instructlab)\n","  Downloading semchunk-2.2.2-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from GitPython>=3.1.42->instructlab) (4.0.12)\n","Collecting haystack-experimental (from haystack-ai>=2.8->instructlab)\n","  Downloading haystack_experimental-0.8.0-py3-none-any.whl.metadata (12 kB)\n","Collecting lazy-imports (from haystack-ai>=2.8->instructlab)\n","  Downloading lazy_imports-0.4.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from haystack-ai>=2.8->instructlab) (10.6.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from haystack-ai>=2.8->instructlab) (3.4.2)\n","Collecting posthog!=3.12.0 (from haystack-ai>=2.8->instructlab)\n","  Downloading posthog-3.23.0-py2.py3-none-any.whl.metadata (3.0 kB)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from haystack-ai>=2.8->instructlab) (2.8.2)\n","Requirement already satisfied: tenacity!=8.4.0 in /usr/local/lib/python3.11/dist-packages (from haystack-ai>=2.8->instructlab) (9.0.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->instructlab) (4.9.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->instructlab) (2025.1.31)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->instructlab) (1.0.7)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->instructlab) (3.10)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.0->instructlab) (0.14.0)\n","Collecting hf-transfer>=0.1.4 (from huggingface_hub[hf_transfer]>=0.1.8->instructlab)\n","  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n","Collecting shortuuid (from instructlab-eval<0.6.0,>=0.5.1->instructlab)\n","  Downloading shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from instructlab-eval<0.6.0,>=0.5.1->instructlab) (1.5.2)\n","Requirement already satisfied: pandas-stubs in /usr/local/lib/python3.11/dist-packages (from instructlab-eval<0.6.0,>=0.5.1->instructlab) (2.2.2.240909)\n","Collecting lm-eval>=0.4.4 (from instructlab-eval<0.6.0,>=0.5.1->instructlab)\n","  Downloading lm_eval-0.4.8-py3-none-any.whl.metadata (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ragas>=0.2.11 (from instructlab-eval<0.6.0,>=0.5.1->instructlab)\n","  Downloading ragas-0.2.14-py3-none-any.whl.metadata (8.5 kB)\n","Collecting yamllint>=1.35.1 (from instructlab-schema>=0.4.2->instructlab)\n","  Downloading yamllint-1.37.0-py3-none-any.whl.metadata (4.3 kB)\n","Collecting datasets>=2.18.0 (from instructlab)\n","  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n","Collecting docling>=2.18.0 (from docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab)\n","  Downloading docling-2.28.4-py3-none-any.whl.metadata (10.0 kB)\n","Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.11/dist-packages (from instructlab-sdg<0.8.0,>=0.7.0->instructlab) (0.3.7)\n","Collecting multiprocess (from datasets>=2.18.0->instructlab)\n","  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec>=2023.5.0 (from huggingface_hub>=0.1.8->huggingface_hub[hf_transfer]>=0.1.8->instructlab)\n","  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: wheel>=0.43 in /usr/local/lib/python3.11/dist-packages (from instructlab-training<0.8.0,>=0.7.0->instructlab) (0.45.1)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from instructlab-training<0.8.0,>=0.7.0->instructlab) (9.0.0)\n","Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from instructlab-training<0.8.0,>=0.7.0->instructlab) (0.60.0)\n","Collecting instructlab-dolomite>=0.2.0 (from instructlab-training<0.8.0,>=0.7.0->instructlab)\n","  Downloading instructlab_dolomite-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n","Collecting aiofiles>=23.2.1 (from instructlab-training<0.8.0,>=0.7.0->instructlab)\n","  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.13.3->instructlab) (1.9.0)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.13.3->instructlab) (0.9.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.13.3->instructlab) (1.3.1)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft>=0.9.0->instructlab) (0.5.3)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit>=3.0.38->instructlab) (0.2.13)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->instructlab) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->instructlab) (2.33.0)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->instructlab) (0.4.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.3.1->instructlab) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.3.1->instructlab) (2.18.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score>=0.1.2->instructlab) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score>=0.1.2->instructlab) (3.9.1)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score>=0.1.2->instructlab) (1.17.0)\n","Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.0->instructlab)\n","  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=3.0.0->instructlab) (1.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=3.0.0->instructlab) (1.14.1)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<2.7.0,>=2.3.0->instructlab)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<2.7.0,>=2.3.0->instructlab)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<2.7.0,>=2.3.0->instructlab)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<2.7.0,>=2.3.0->instructlab)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<2.7.0,>=2.3.0->instructlab)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<2.7.0,>=2.3.0->instructlab)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch<2.7.0,>=2.3.0->instructlab)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<2.7.0,>=2.3.0->instructlab)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<2.7.0,>=2.3.0->instructlab)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.3.0->instructlab) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.3.0->instructlab) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.3.0->instructlab) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<2.7.0,>=2.3.0->instructlab)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.3.0->instructlab) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.3.0->instructlab) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<2.7.0,>=2.3.0->instructlab) (1.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.41.2->instructlab) (2024.11.6)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.16.4->instructlab) (0.4.0)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb>=0.16.4->instructlab) (4.3.7)\n","Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.16.4->instructlab) (5.29.4)\n","Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.16.4->instructlab) (2.25.0)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb>=0.16.4->instructlab) (1.3.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb>=0.16.4->instructlab) (75.2.0)\n","Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.38.0,>=1.37.27->boto3>=1.35.96->instructlab) (2.3.0)\n","Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab) (4.13.3)\n","Collecting docling-ibm-models<4.0.0,>=3.4.0 (from docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab)\n","  Downloading docling_ibm_models-3.4.1-py3-none-any.whl.metadata (7.4 kB)\n","Collecting docling-parse<5.0.0,>=4.0.0 (from docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab)\n","  Downloading docling_parse-4.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n","Collecting easyocr<2.0,>=1.7 (from docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab)\n","  Downloading easyocr-1.7.2-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab) (1.2.0)\n","Requirement already satisfied: lxml<6.0.0,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab) (5.3.1)\n","Collecting marko<3.0.0,>=2.1.2 (from docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab)\n","  Downloading marko-2.1.2-py3-none-any.whl.metadata (4.5 kB)\n","Requirement already satisfied: openpyxl<4.0.0,>=3.1.5 in /usr/local/lib/python3.11/dist-packages (from docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab) (3.1.5)\n","Requirement already satisfied: pluggy<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab) (1.5.0)\n","Collecting pylatexenc<3.0,>=2.10 (from docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab)\n","  Downloading pylatexenc-2.10.tar.gz (162 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pypdfium2<5.0.0,>=4.30.0 (from docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab)\n","  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting python-docx<2.0.0,>=1.1.2 (from docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab)\n","  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n","Collecting python-pptx<2.0.0,>=1.0.2 (from docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab)\n","  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n","Collecting rtree<2.0.0,>=1.3.0 (from docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab)\n","  Downloading rtree-1.4.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n","Collecting tesserocr<3.0.0,>=2.7.1 (from docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab)\n","  Downloading tesserocr-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (10 kB)\n","Collecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.100.0->llama_cpp_python[server]==0.3.6->instructlab)\n","  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.18.0->instructlab) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.18.0->instructlab) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.18.0->instructlab) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.18.0->instructlab) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.18.0->instructlab) (6.3.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.18.0->instructlab) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.18.0->instructlab) (1.18.3)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->GitPython>=3.1.42->instructlab) (5.0.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama_cpp_python==0.3.6->llama_cpp_python[server]==0.3.6->instructlab) (3.0.2)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core>=2.10.0->docling-core[chunking]>=2.10.0->instructlab) (2024.10.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core>=2.10.0->docling-core[chunking]>=2.10.0->instructlab) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core>=2.10.0->docling-core[chunking]>=2.10.0->instructlab) (0.24.0)\n","Collecting evaluate (from lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab)\n","  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n","Collecting jsonlines (from lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab)\n","  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n","Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab) (2.10.2)\n","Collecting pybind11>=2.6.2 (from lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab)\n","  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n","Collecting pytablewriter (from lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab)\n","  Downloading pytablewriter-1.2.1-py3-none-any.whl.metadata (38 kB)\n","Collecting sacrebleu>=1.5.0 (from lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab)\n","  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sqlitedict (from lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab)\n","  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting tqdm-multiprocess (from lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab)\n","  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: zstandard in /usr/local/lib/python3.11/dist-packages (from lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab) (0.23.0)\n","Collecting word2number (from lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab)\n","  Downloading word2number-1.1.zip (9.7 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.3.1->instructlab) (0.1.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.18.0->instructlab) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.18.0->instructlab) (2025.2)\n","Collecting monotonic>=1.5 (from posthog!=3.12.0->haystack-ai>=2.8->instructlab)\n","  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n","Collecting backoff>=1.10.0 (from posthog!=3.12.0->haystack-ai>=2.8->instructlab)\n","  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings>=2.0.1->llama_cpp_python[server]==0.3.6->instructlab) (1.1.0)\n","Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab) (0.9.0)\n","Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (from ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab) (0.3.22)\n","Requirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (from ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab) (0.3.49)\n","Collecting langchain-community (from ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab)\n","  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n","Requirement already satisfied: langchain_openai in /usr/local/lib/python3.11/dist-packages (from ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab) (0.3.12)\n","Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab) (1.6.0)\n","Collecting appdirs (from ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab)\n","  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.18.0->instructlab) (3.4.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=3.0.0->instructlab) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=3.0.0->instructlab) (3.6.0)\n","Collecting mpire[dill] (from semchunk<3.0.0,>=2.2.0->docling-core[chunking]>=2.10.0->instructlab)\n","  Downloading mpire-2.10.2-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->docling-core>=2.10.0->docling-core[chunking]>=2.10.0->instructlab) (1.5.4)\n","Collecting pathspec>=0.5.3 (from yamllint>=1.35.1->instructlab-schema>=0.4.2->instructlab)\n","  Using cached pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n","INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->instructlab-training<0.8.0,>=0.7.0->instructlab) (0.43.0)\n","Requirement already satisfied: types-pytz>=2022.1.1 in /usr/local/lib/python3.11/dist-packages (from pandas-stubs->instructlab-eval<0.6.0,>=0.5.1->instructlab) (2025.2.0.20250326)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab) (2.6)\n","Collecting jsonlines (from lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab)\n","  Downloading jsonlines-3.1.0-py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: opencv-python-headless<5.0.0.0,>=4.6.0.66 in /usr/local/lib/python3.11/dist-packages (from docling-ibm-models<4.0.0,>=3.4.0->docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab) (4.11.0.86)\n","Requirement already satisfied: torchvision<1,>=0 in /usr/local/lib/python3.11/dist-packages (from docling-ibm-models<4.0.0,>=3.4.0->docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab) (0.21.0+cu124)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from easyocr<2.0,>=1.7->docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab) (0.25.2)\n","Collecting python-bidi (from easyocr<2.0,>=1.7->docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab)\n","  Downloading python_bidi-0.6.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.11/dist-packages (from easyocr<2.0,>=1.7->docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab) (2.0.7)\n","Collecting pyclipper (from easyocr<2.0,>=1.7->docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab)\n","  Downloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n","Collecting ninja (from easyocr<2.0,>=1.7->docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab)\n","  Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n","Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core->ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab) (0.3.22)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core->ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab) (1.33)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl<4.0.0,>=3.1.5->docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab) (2.0.0)\n","Collecting XlsxWriter>=0.5.7 (from python-pptx<2.0.0,>=1.0.2->docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab)\n","  Downloading XlsxWriter-3.2.2-py3-none-any.whl.metadata (2.8 kB)\n","Collecting portalocker (from sacrebleu>=1.5.0->lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab)\n","  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n","Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.5.0->lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab) (0.4.6)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain->ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab) (2.0.40)\n","Collecting langchain-core (from ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab)\n","  Downloading langchain_core-0.3.51-py3-none-any.whl.metadata (5.9 kB)\n","Collecting langchain (from ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab)\n","  Downloading langchain-0.3.23-py3-none-any.whl.metadata (7.8 kB)\n","Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community->ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab) (0.6.7)\n","Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community->ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab)\n","  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n","Collecting langchain-text-splitters (from instructlab-sdg<0.8.0,>=0.7.0->instructlab)\n","  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n","Collecting DataProperty<2,>=1.1.0 (from pytablewriter->lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab)\n","  Downloading DataProperty-1.1.0-py3-none-any.whl.metadata (11 kB)\n","Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab)\n","  Downloading mbstrdecoder-1.1.4-py3-none-any.whl.metadata (4.3 kB)\n","Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab)\n","  Downloading pathvalidate-3.2.3-py3-none-any.whl.metadata (12 kB)\n","Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab)\n","  Downloading tabledata-1.3.4-py3-none-any.whl.metadata (3.7 kB)\n","Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab)\n","  Downloading tcolorpy-0.1.7-py3-none-any.whl.metadata (6.3 kB)\n","Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab)\n","  Downloading typepy-1.3.4-py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab) (3.26.1)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab) (0.9.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core->ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab) (3.0.0)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core->ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab) (3.10.16)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core->ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab) (1.0.0)\n","Requirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.11/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm-eval>=0.4.4->instructlab-eval<0.6.0,>=0.5.1->instructlab) (5.2.0)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain->ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab) (3.1.1)\n","Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr<2.0,>=1.7->docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab) (2.37.0)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr<2.0,>=1.7->docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab) (2025.3.30)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr<2.0,>=1.7->docling>=2.18.0->docling[tesserocr]>=2.18.0; sys_platform != \"darwin\"->instructlab-sdg<0.8.0,>=0.7.0->instructlab) (0.4)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas>=0.2.11->instructlab-eval<0.6.0,>=0.5.1->instructlab) (1.0.0)\n","Downloading instructlab-0.24.3-py3-none-any.whl (285 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.5/285.5 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading boto3-1.37.27-py3-none-any.whl (139 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.6/139.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading click_didyoumean-0.3.1-py3-none-any.whl (3.6 kB)\n","Downloading docling_core-2.25.0-py3-none-any.whl (121 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gguf-0.14.0-py3-none-any.whl (76 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading haystack_ai-2.12.0-py3-none-any.whl (482 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m482.8/482.8 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading instructlab_eval-0.5.1-py3-none-any.whl (70 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.9/70.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading instructlab_quantize-0.1.0-py3-none-any.whl (2.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading instructlab_schema-0.4.2-py3-none-any.whl (18 kB)\n","Downloading instructlab_sdg-0.7.3-py3-none-any.whl (85 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading datasets-2.21.0-py3-none-any.whl (527 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading instructlab_training-0.7.0-py3-none-any.whl (54 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.0/278.0 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydantic_yaml-1.4.0-py3-none-any.whl (17 kB)\n","Downloading ruamel.yaml-0.18.10-py3-none-any.whl (117 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.7/117.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.6/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:02:28\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trl-0.14.0-py3-none-any.whl (313 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.9/313.9 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xdg_base_dirs-6.0.2-py3-none-any.whl (4.7 kB)\n","Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n","Downloading botocore-1.37.27-py3-none-any.whl (13.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading docling-2.28.4-py3-none-any.whl (160 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.4/160.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading instructlab_dolomite-0.2.0-py3-none-any.whl (63 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n","Downloading latex2mathml-3.77.0-py3-none-any.whl (73 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lm_eval-0.4.8-py3-none-any.whl (3.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading posthog-3.23.0-py2.py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n","Downloading ragas-0.2.14-py3-none-any.whl (187 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.2/187.2 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading s3transfer-0.11.4-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading semchunk-2.2.2-py3-none-any.whl (10 kB)\n","Downloading sse_starlette-2.2.1-py3-none-any.whl (10 kB)\n","Downloading starlette_context-0.3.6-py3-none-any.whl (12 kB)\n","Downloading typer-0.12.5-py3-none-any.whl (47 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading yamllint-1.37.0-py3-none-any.whl (68 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.7/68.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading haystack_experimental-0.8.0-py3-none-any.whl (59 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lazy_imports-0.4.0-py3-none-any.whl (12 kB)\n","Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\n","Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n","Downloading docling_ibm_models-3.4.1-py3-none-any.whl (80 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading docling_parse-4.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading easyocr-1.7.2-py3-none-any.whl (2.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n","Downloading marko-2.1.2-py3-none-any.whl (42 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n","Using cached pathspec-0.12.1-py3-none-any.whl (31 kB)\n","Downloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading rtree-1.4.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (541 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m541.1/541.1 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading starlette-0.46.1-py3-none-any.whl (71 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tesserocr-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl (5.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n","Downloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain-0.3.23-py3-none-any.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n","Downloading langchain_core-0.3.51-py3-none-any.whl (423 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.3/423.3 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pytablewriter-1.2.1-py3-none-any.whl (91 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n","Downloading DataProperty-1.1.0-py3-none-any.whl (27 kB)\n","Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n","Downloading mbstrdecoder-1.1.4-py3-none-any.whl (7.9 kB)\n","Downloading pathvalidate-3.2.3-py3-none-any.whl (24 kB)\n","Downloading tabledata-1.3.4-py3-none-any.whl (11 kB)\n","Downloading tcolorpy-0.1.7-py3-none-any.whl (8.1 kB)\n","Downloading typepy-1.3.4-py3-none-any.whl (31 kB)\n","Downloading XlsxWriter-3.2.2-py3-none-any.whl (165 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.1/165.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mpire-2.10.2-py3-none-any.whl (272 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.8/272.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hUsing cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n","Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n","Downloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (969 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m969.6/969.6 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_bidi-0.6.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (292 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.9/292.9 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.\n","    unknown package:\n","        Expected sha256 165764f44ef8c61fcdfdfdbe769d687e06374059fbb388b6c89ecb0e28793a6f\n","             Got        ece60e90996d212b22f18dcb35bd579a7c7fc745063b4049e4889ca37c9acb73\n","\u001b[0m\u001b[31m\n","\u001b[0mCollecting asyncio\n","  Downloading asyncio-3.4.3-py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n","Downloading asyncio-3.4.3-py3-none-any.whl (101 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: asyncio\n","Successfully installed asyncio-3.4.3\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["asyncio"]},"id":"922779bec0eb4eb385ebfa8b973f631d"}},"metadata":{}}],"source":["\n","!pip install llama-index llama-parse \"psycopg[binary]\" openai\n","!pip install langchain langchain-openai\n","!pip install instructlab\n","!pip install asyncio nest_asyncio\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KSJoT3YhggtP"},"outputs":[],"source":["#Set openai api_key value\n","import os\n","os.environ['OPENAI_API_KEY'] = \"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\"\n"]},{"cell_type":"markdown","metadata":{"id":"dv7a7zbbXnUD"},"source":["**Step 2: Define all classes**\n","\n","2.1: Create Document Processsor class\n","\n","(Note, there is a new potential enhancement to this step, called PageIndex\n","(see this github repository's readMe page for details: https://github.com/VectifyAI/PageIndex)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yIjwGCjeXrq5"},"outputs":[],"source":["# Cell 1: Define EnhancedDocumentProcessor class\n","import os\n","import re\n","import json\n","import logging\n","from typing import List, Dict, Any, Optional\n","from datetime import datetime\n","\n","# LlamaParse for advanced PDF handling\n","from llama_parse import LlamaParse\n","\n","# LlamaIndex for document structuring and retrieval\n","from llama_index.core import Document\n","from llama_index.core.node_parser import SentenceSplitter\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","\n","class EnhancedDocumentProcessor:\n","    \"\"\"Process RFQ documents with hierarchical awareness and page tracking\"\"\"\n","\n","    def __init__(self, llama_api_key: str):\n","        \"\"\"Initialize with necessary API keys and configurations\"\"\"\n","        self.parser = LlamaParse(\n","            api_key=llama_api_key,\n","            result_type=\"markdown\",  # Get structured markdown output\n","            max_request_size=20000,  # Handle larger documents\n","            verbose=True,\n","            include_metadata=True    # Ensure metadata like page numbers are included\n","        )\n","        self.node_parser = SentenceSplitter(\n","            chunk_size=512,           # Balance between context and specificity\n","            chunk_overlap=50,         # Maintain continuity between chunks\n","            paragraph_separator=\"\\n\\n\"\n","        )\n","\n","    async def process_document(self, pdf_path: str) -> List[Dict[str, Any]]:\n","        \"\"\"Process document and extract structured nodes with rich metadata including page numbers\"\"\"\n","        logging.info(f\"Processing document: {pdf_path}\")\n","\n","        # Extract file metadata\n","        filename = os.path.basename(pdf_path)\n","        document_id = os.path.splitext(filename)[0]\n","        nodes = []\n","\n","        try:\n","            # Parse the PDF with LlamaParse\n","            parsed_document = await self.parser.aload_data(pdf_path)\n","            logging.info(f\"Document parsed successfully: {len(parsed_document)} sections\")\n","\n","            # ADD DIAGNOSTIC Script HERE\n","            with open(pdf_path, 'rb') as f:\n","                content = f.read()\n","            logging.info(f\"File size: {len(content)} bytes\")\n","            logging.info(f\"Parsed content length: {sum(len(str(item)) for item in parsed_document)}\")\n","            logging.info(f\"First 500 chars: {str(parsed_document[0])[:500] if parsed_document else 'EMPTY'}\")\n","            # Try basic regex directly on raw text\n","            import re\n","            raw_text = \" \".join([str(item) for item in parsed_document])\n","            shall_count = len(re.findall(r'\\bshall\\b', raw_text, re.IGNORECASE))\n","            must_count = len(re.findall(r'\\bmust\\b', raw_text, re.IGNORECASE))\n","            will_count = len(re.findall(r'\\bwill\\b', raw_text, re.IGNORECASE))\n","            logging.info(f\"Found {shall_count} shall, {must_count} must, {will_count} will statements\")\n","            # END DIAGNOSTIC CODE\n","\n","            # Extract document structure with section hierarchy and page numbers\n","            structured_sections = self._extract_document_structure(parsed_document)\n","            logging.info(f\"Extracted {len(structured_sections)} structured sections\")\n","\n","            # Create nodes with enhanced metadata\n","            nodes = self._create_nodes_with_metadata(structured_sections, document_id)\n","            logging.info(f\"Created {len(nodes)} context-rich nodes\")\n","\n","        except Exception as e:\n","            logging.error(f\"Error in primary parsing: {str(e)}\")\n","            # Fallback to simpler parsing\n","            parsed_document = await self.parser.aload_data(pdf_path)\n","\n","            logging.info(f\"Document parsed successfully: {len(parsed_document)} sections\")\n","            # Extract document structure with section hierarchy\n","            structured_sections = self._extract_document_structure(parsed_document)\n","            logging.info(f\"Extracted {len(structured_sections)} structured sections\")\n","\n","            # Create nodes with enhanced metadata\n","            nodes = self._create_nodes_with_metadata(structured_sections, document_id)\n","            logging.info(f\"Created {len(nodes)} context-rich nodes\")\n","\n","        return nodes\n","\n","    def _extract_document_structure(self, parsed_document: List[Any]) -> List[Dict[str, Any]]:\n","        \"\"\"Extract hierarchical section information and page numbers from parsed document\"\"\"\n","        structured_sections = []\n","        current_sections = {\"level1\": None, \"level2\": None, \"level3\": None}\n","\n","        # Track the current header information\n","        current_header = \"Unknown Header\"\n","        current_header_num = \"\"\n","\n","        for item in parsed_document:\n","            # Handle different response formats from LlamaParse\n","            if isinstance(item, dict):\n","                content = item.get('text', '')\n","                metadata = item.get('metadata', {})\n","            else:\n","                content = item.text\n","                metadata = item.metadata\n","\n","            # Extract page number - add default if not available\n","            page_number = 1\n","            if hasattr(metadata, 'get'):\n","                page_number = metadata.get('page_number', 1)\n","            elif hasattr(metadata, 'page_label'):\n","                page_number = metadata.page_label\n","                if isinstance(page_number, str) and page_number.isdigit():\n","                    page_number = int(page_number)\n","                else:\n","                    page_number = 1\n","\n","            # Detect section headers\n","            section_header = self._detect_section_header(content)\n","            header_text = section_header if section_header else current_header\n","\n","            # Extract requirement header number if present\n","            header_number = self._extract_header_number(header_text)\n","            if header_number:\n","                current_header_num = header_number\n","\n","            if section_header:\n","                # Determine section level from formatting and content\n","                level = self._determine_section_level(section_header, content)\n","\n","                # Update current section hierarchy\n","                if level == 1:\n","                    current_sections[\"level1\"] = section_header\n","                    current_sections[\"level2\"] = None\n","                    current_sections[\"level3\"] = None\n","                    current_header = section_header\n","                elif level == 2:\n","                    current_sections[\"level2\"] = section_header\n","                    current_sections[\"level3\"] = None\n","                    current_header = section_header\n","                elif level == 3:\n","                    current_sections[\"level3\"] = section_header\n","                    current_header = section_header\n","\n","            # Build full section path for context\n","            section_path = self._build_section_path(current_sections)\n","\n","            # Store structured section with hierarchy information and page number\n","            level = max((i + 1 for i, s in enumerate(current_sections.values()) if s is not None), default=1)\n","            structured_sections.append({\n","                \"text\": content,\n","                \"section\": section_path,\n","                \"level\": level,\n","                \"page_number\": page_number,\n","                \"header\": current_header,\n","                \"header_number\": current_header_num,\n","                \"metadata\": metadata\n","            })\n","\n","        return structured_sections\n","\n","    def _detect_section_header(self, text: str) -> Optional[str]:\n","        \"\"\"Enhanced section header detection for various document structures\"\"\"\n","        # Expanded patterns to match more document formats\n","        section_patterns = [\n","            # Standard section formats\n","            r'^(?:[A-Z]\\.|\\d+\\.)\\s*([A-Za-z\\s]+)',              # A. Section Name or 1. Section Name\n","            r'^(?:Section|SECTION)\\s+[A-Z]+[\\.:]?\\s*(.+)',      # Section A: Name\n","            r'^(?:[IVX]+\\.|\\d+\\.\\d+\\.?)\\s+([A-Za-z\\s]+)',       # IV. Name or 1.2 Name\n","            # Specialized formats common in SOWs\n","            r'^(?:Task|TASK)\\s+\\d+(?:\\.\\d+)?[\\.:]?\\s*(.+)',     # Task 1: Name or Task 1.2: Name\n","            r'^(?:Deliverable|DELIVERABLE)\\s+\\d+[\\.:]?\\s*(.+)', # Deliverable 1: Name\n","            r'^[A-Z][A-Z\\s]+:',                                 # PURPOSE: or BACKGROUND:\n","            r'^\\s*[A-Z][A-Za-z\\s]+\\s*$'                         # Single line all caps or title case\n","        ]\n","\n","        for pattern in section_patterns:\n","            matches = re.search(pattern, text, re.MULTILINE)\n","            if matches:\n","                # Clean up and normalize the header text\n","                header = matches.group(0).strip()\n","                # Store full section path information\n","                return header\n","\n","        return None\n","\n","    def _extract_header_number(self, header_text: str) -> str:\n","        \"\"\"Extract numerical identifier from a header if present\"\"\"\n","        patterns = [\n","            r'(\\d+\\.\\d+(?:\\.\\d+)?)',  # Matches 1.2 or 1.2.3\n","            r'([A-Z]\\.\\d+)',          # Matches C.1\n","            r'Task\\s+(\\d+(?:\\.\\d+)?)', # Matches Task 1 or Task 1.2\n","            r'Section\\s+(\\d+(?:\\.\\d+)?)', # Matches Section 1 or Section 1.2\n","            r'^(\\d+)\\.'               # Matches 1. at the beginning of a string\n","        ]\n","\n","        for pattern in patterns:\n","            match = re.search(pattern, header_text)\n","            if match:\n","                return match.group(1)\n","\n","        return \"\"\n","\n","    def _determine_section_level(self, header: str, content: str) -> int:\n","        \"\"\"Determine section level from header format\"\"\"\n","        # Check for heading level indicators\n","        if re.match(r'^C\\.\\d+$|^\\d+\\.$|^Task\\s+\\d+$|^Section\\s+\\d+$', header):\n","            return 1  # Top level\n","        elif re.match(r'^C\\.\\d+\\.\\d+$|^\\d+\\.\\d+$|^Task\\s+\\d+\\.\\d+$', header):\n","            return 2  # Sub-section\n","        elif re.match(r'^C\\.\\d+\\.\\d+\\.\\d+$|^\\d+\\.\\d+\\.\\d+$', header):\n","            return 3  # Sub-sub-section\n","\n","        # Fallback on heading style if available\n","        if content.startswith('# '):\n","            return 1\n","        elif content.startswith('## '):\n","            return 2\n","        elif content.startswith('### '):\n","            return 3\n","\n","        return 1  # Default to top level if unsure\n","\n","    def _build_section_path(self, sections: Dict[str, Optional[str]]) -> str:\n","        \"\"\"Build full section path for hierarchical context\"\"\"\n","        path_parts = []\n","        for level in [\"level1\", \"level2\", \"level3\"]:\n","            if sections[level]:\n","                path_parts.append(sections[level])\n","\n","        return \" > \".join(path_parts) if path_parts else \"Unknown Section\"\n","\n","    def _create_nodes_with_metadata(self, structured_sections: List[Dict], document_id: str) -> List[Dict]:\n","        \"\"\"Create content nodes with rich metadata from structured sections including page numbers\"\"\"\n","        enriched_nodes = []\n","\n","        for section in structured_sections:\n","            # Skip empty sections\n","            if not section[\"text\"].strip():\n","                continue\n","\n","            # Create document for this section\n","            doc = Document(\n","                text=section[\"text\"],\n","                metadata={\n","                    \"section_path\": section[\"section\"],\n","                    \"section_level\": section[\"level\"],\n","                    \"document_id\": document_id,\n","                    \"page_number\": section[\"page_number\"],\n","                    \"header\": section[\"header\"],\n","                    \"header_number\": section[\"header_number\"],\n","                    \"processing_date\": datetime.now().isoformat(),\n","                }\n","            )\n","\n","            # Parse into nodes with context preserved\n","            nodes = self.node_parser.get_nodes_from_documents([doc])\n","\n","            # Add original metadata to each node\n","            for node in nodes:\n","                node_dict = node.to_dict()\n","                node_dict[\"metadata\"].update(section.get(\"metadata\", {}))\n","                enriched_nodes.append(node_dict)\n","\n","        return enriched_nodes\n"]},{"cell_type":"markdown","metadata":{"id":"tu1Ky1AKxMcx"},"source":["2.2: Create Task Extraction class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Il4Vq5QBr7g"},"outputs":[],"source":["# Updated Step 2: Task extraction tool build\n","\n","import json\n","import logging\n","from typing import List, Dict, Any\n","import openai\n","from openai import AsyncOpenAI\n","import os\n","import re\n","from difflib import SequenceMatcher\n","from tenacity import retry, stop_after_attempt, wait_exponential\n","\n","class TaskExtractionAgent:\n","    \"\"\"Extract requirements from RFQ documents with page number and header tracking\"\"\"\n","    def __init__(self, api_key: str, model: str = \"gpt-4o\"):\n","        \"\"\"Initialize with OpenAI API key and model\"\"\"\n","        self.api_key = api_key\n","        self.model = model\n","        openai.api_key = api_key  # Set the API key globally\n","        self.req_counter = 1  # Sequential requirement counter\n","        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","\n","    def _extract_technical_skill_area(self, task_text: str) -> str:\n","        \"\"\"Identify technical skill area based on requirement text\"\"\"\n","        text_lower = task_text.lower()\n","\n","        # Extended Technical Skill Areas mapping\n","        skill_areas = {\n","            \"Data Management\": [\"data\", \"database\", \"storage\", \"repository\", \"files\"],\n","            \"Analytics\": [\"analysis\", \"analyze\", \"analytics\", \"statistics\", \"trends\"],\n","            \"Training\": [\"train\", \"education\", \"curriculum\", \"webinar\", \"instruction\"],\n","            \"Project Management\": [\"manage\", \"coordinate\", \"schedule\", \"plan\", \"timeline\"],\n","            \"Technical Support\": [\"support\", \"assist\", \"help desk\", \"guidance\"],\n","            \"Quality Improvement\": [\"quality\", \"improvement\", \"performance\", \"measure\"],\n","            \"Documentation\": [\"document\", \"report\", \"write\", \"prepare\", \"draft\"],\n","            \"Meeting Facilitation\": [\"meeting\", \"facilitate\", \"agenda\", \"discussion\"],\n","            \"Research\": [\"research\", \"study\", \"investigate\", \"literature\"],\n","            \"Compliance\": [\"comply\", \"compliance\", \"regulation\", \"requirement\"],\n","            \"Security\": [\"security\", \"protection\", \"confidential\", \"privacy\"],\n","\n","            # Design Services\n","            \"Design Services\": [\"branding\", \"identity\", \"graphic\", \"illustration\", \"human-centered\",\n","                              \"multimedia\", \"video\", \"packaging\", \"print\", \"product\", \"industrial\",\n","                              \"web\", \"mobile\", \"design\"],\n","\n","            # Financial & Accounting Services\n","            \"Financial & Accounting Services\": [\"accounting\", \"bookkeeping\", \"audit\", \"assurance\",\n","                                             \"banking\", \"payment\", \"cfo\", \"financial planning\",\n","                                             \"risk\", \"compliance\", \"strategy\", \"transformation\",\n","                                             \"erp\", \"forensic\", \"fraud\", \"grants\", \"investment\",\n","                                             \"treasury\", \"merger\", \"acquisition\", \"payroll\",\n","                                             \"procurement\", \"contract\", \"tax\"],\n","\n","            # Human Resources Services\n","            \"Human Resources Services\": [\"compensation\", \"benefits\", \"diversity\", \"employee\",\n","                                      \"relations\", \"conflict\", \"hr policy\", \"hris\", \"hr technology\",\n","                                      \"outplacement\", \"offboarding\", \"performance\", \"staffing\",\n","                                      \"talent\", \"succession\", \"career\", \"recruitment\"],\n","\n","            # IT Services\n","            \"IT Services\": [\"artificial intelligence\", \"machine learning\", \"blockchain\", \"distributed ledger\",\n","                         \"cloud\", \"cybersecurity\", \"data\", \"analytics\", \"devops\", \"automation\",\n","                         \"emerging\", \"healthcare it\", \"infrastructure\", \"networking\", \"internet of things\",\n","                         \"iot\", \"it consulting\", \"strategy\", \"governance\", \"support\", \"managed services\",\n","                         \"quantum\", \"software\", \"application\", \"development\"],\n","\n","            # Legal Services\n","            \"Legal Services\": [\"dispute resolution\", \"adr\", \"contract law\", \"corporate governance\",\n","                           \"data privacy\", \"employment law\", \"government contracting\", \"intellectual property\",\n","                           \"ip\", \"litigation\", \"ediscovery\", \"m&a legal\", \"regulatory compliance\"],\n","\n","            # Management Services\n","            \"Management Services\": [\"agile\", \"change management\", \"governance\", \"operations\", \"performance\",\n","                                \"process improvement\", \"procurement\", \"supply\", \"project\", \"program\",\n","                                \"strategy\", \"business planning\"],\n","\n","            # Marketing & PR Services\n","            \"Marketing & PR Services\": [\"advertising\", \"brand\", \"content\", \"seo\", \"digital marketing\",\n","                                     \"event\", \"promotions\", \"market research\", \"public relations\"],\n","\n","            # Training & Communications\n","            \"Training & Communications\": [\"change communications\", \"crisis\", \"elearning\", \"instructional\",\n","                                       \"internal communications\", \"knowledge management\", \"technical writing\",\n","                                       \"leadership\", \"management training\", \"media training\", \"public speaking\",\n","                                       \"onboarding\", \"orientation\", \"soft skills\", \"technical training\"]\n","        }\n","\n","        # Find matching skill area\n","        for area, keywords in skill_areas.items():\n","            if any(keyword in text_lower for keyword in keywords):\n","                return area\n","        # Default if no match found\n","        return \"Process Implementation\"\n","\n","    def _generate_brief_requirement(self, task_text: str) -> str:\n","        \"\"\"Generate brief requirement description from full text\"\"\"\n","        # Try to extract core requirement using patterns\n","        patterns = [\n","            r'shall\\s+([^.,;]+)',\n","            r'will\\s+([^.,;]+)',\n","            r'must\\s+([^.,;]+)',\n","            r'required to\\s+([^.,;]+)'\n","        ]\n","\n","        for pattern in patterns:\n","            match = re.search(pattern, task_text, re.IGNORECASE)\n","            if match:\n","                brief = match.group(1).strip()\n","                # Capitalize first letter and ensure it ends with proper punctuation\n","                brief = brief[0].upper() + brief[1:]\n","                if len(brief) > 10 and len(brief) <= 80:\n","                    return brief\n","\n","        # If no good matches, take first X characters and clean up\n","        if len(task_text) > 80:\n","            brief = task_text[:77] + \"...\"\n","        else:\n","            brief = task_text\n","\n","        # Clean up any partial sentences\n","        brief = re.sub(r'^\\s*the\\s+contractor\\s+shall\\s+', '', brief, flags=re.IGNORECASE)\n","\n","        return brief\n","\n","    def validate_tasks(self, tasks: List[Dict[str, Any]], document_id: str, full_document_text: str) -> List[Dict[str, Any]]:\n","        valid_tasks = []\n","        seen_texts = set()\n","\n","        task_id=0\n","        for task in tasks:\n","            # Less restrictive filtering\n","            if len(task.get(\"task_text\", \"\")) > 5:\n","                task_text = task.get(\"task_text\", \"\").lower().strip()\n","                if task_text not in seen_texts:\n","                    seen_texts.add(task_text)\n","\n","                    # Calculate task_index using fuzzy matching\n","                    task_index = self.find_text_index(full_document_text, task.get(\"task_text\", \"\"))\n","                    if task_index == -1:\n","                        logging.warning(f\"Task text not found in document: {task.get('task_text', '')}\")\n","                    else:\n","                        task_id += 1\n","                    # Create task in new required format\n","                    formatted_task = {\n","                        \"task_id\": task_id,\n","                        \"document_id\": document_id,\n","                        \"task_text\": task.get(\"task_text\", \"\"),\n","                        \"task_index\": task_index,\n","                    }\n","\n","                    valid_tasks.append(formatted_task)\n","        return valid_tasks\n","\n","    \"\"\" prior code for 'find_text_index':\n","        def find_text_index(self, full_document_text: str, task_text: str) -> int:\n","        \"\"\" \"\"\"Find the index of task_text within full_document_text using fuzzy matching.\"\"\" \"\"\"\n","        normalized_document = full_document_text.lower().replace(\"\\n\", \" \").strip()\n","        normalized_task = task_text.lower().replace(\"\\n\", \" \").strip()\n","\n","        # Use SequenceMatcher to find the best match\n","        match = SequenceMatcher(None, normalized_document, normalized_task).find_longest_match(0, len(normalized_document), 0, len(normalized_task))\n","\n","        if match.size > 0:\n","            return match.a  # Return start index of match\n","        else:\n","            return -1  # Return -1 if no match found\n","            \"\"\"\n","    def find_text_index(self, full_document_text: str, task_text: str) -> int:\n","        \"\"\"Find the index of task_text within full_document_text reliably.\"\"\"\n","\n","        # Normalize both texts, remove extra spaces and newlines\n","        normalized_document = ' '.join(full_document_text.lower().split())\n","        normalized_task = ' '.join(task_text.lower().split())\n","\n","        # Start with an exact substring search\n","        exact_index = normalized_document.find(normalized_task)\n","        if exact_index >= 0:\n","            return exact_index\n","\n","        # If no exact match, perform fuzzy matching with higher threshold\n","        matcher = SequenceMatcher(None, normalized_document, normalized_task)\n","        match = matcher.find_longest_match(0, len(normalized_document), 0, len(normalized_task))\n","\n","        # Set stronger threshold (match must account for at least 90% of task_text length)\n","        if match.size / len(normalized_task) > 0.9:\n","            return match.a\n","        else:\n","            # As an extra safety net, return -1 if no sufficiently good match is found\n","            # This clearly signals problematic matches\n","            return -1\n","\n","    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1, max=10))\n","    async def extract_tasks(self, document_node: Dict[str, Any], full_document_text: str) -> List[Dict[str, Any]]:\n","        \"\"\"Extract requirements with page numbers and header context\"\"\"\n","        # Get text and metadata from the node\n","        text = document_node.get(\"text\", \"\")\n","        metadata = document_node.get(\"metadata\", {})\n","        document_id = metadata.get(\"document_id\", \"Unknown Document\")\n","        section_path = metadata.get(\"section_path\", \"Unknown Section\")\n","        header = metadata.get(\"header\", \"Unknown Section\")\n","        page_number = metadata.get(\"page_number\", 1)\n","\n","        # Diagnostic information about the input text\n","        logging.info(f\"Processing text from page {page_number}, length {len(text)} chars\")\n","        logging.info(f\"From section: {header}\")\n","\n","        # Check for common requirement indicators in text\n","        shall_count = len(re.findall(r'\\bshall\\b', text, re.IGNORECASE))\n","        must_count = len(re.findall(r'\\bmust\\b', text, re.IGNORECASE))\n","        will_count = len(re.findall(r'\\bwill\\b', text, re.IGNORECASE))\n","        logging.info(f\"Text contains: {shall_count} 'shall', {must_count} 'must', {will_count} 'will' statements\")\n","\n","        # Initialize tasks list\n","        tasks = []\n","\n","        # Use enhanced prompt focused on clear requirements\n","        system_prompt = \"\"\"\n","          You are an expert contract analyst, extracting verbatim tasks precisely assigned to a contractor within procurement and contract documents.\n","\n","          Extract every sentence or bullet exactly from the provided text that clearly assigns, instructs, describes, implies, or outlines any task, responsibility, or action to be performed by the Contractor.\n","\n","          Rules for extraction:\n","          - Include EACH sentence or bullet verbatim exactly as it appears in the source text, without summarizing, paraphrasing, adding or omitting words, altering capitalization, bullet symbols, punctuation, spacing, or formatting.\n","          - Include sentences defined explicitly with phrases like \"The Contractor shall/should/must/will/may...\" etc.\n","          - Include sentences that clearly imply or instruct actions the contractor might take, even without explicitly including the word \"Contractor\".\n","          - Include short imperative-form tasks explicitly directed toward the contractor.\n","          - Include tasks with conditionals/qualifiers (\"if applicable,\" \"where appropriate,\" \"as needed,\" etc.).\n","          - Include phrases explicitly mentioning collaboration or coordination (\"shall collaborate with,\" \"shall coordinate with,\" \"shall work with,\" etc.).\n","          - Ensure compound sentences (multiple tasks linked within one sentence) are extracted as ONE complete sentence without breaking them apart.\n","          - Include any variations/substitutions explicitly indicating the Contractor (\"The Contractor\", \"Implementation Contractor\", \"Contractor staff,\" etc.).\n","\n","          Examples explicitly illustrating various extraction cases (EXACT FORMAT REQUIRED):\n","          - The Contractor shall produce materials and manage these products as syndicated content that can be distributed through multiple partner channels and mediums.\n","          - The Contractor shall work with other CMS contractors to ensure data integration and reporting quality metrics from CMMI models to other CMS programs are performed as needed.\n","          - Prepare a Final Report upon project completion.\n","          - Support the CMS actuarial certification review processes and the process of submitting the model for approval of expansion by the Secretary\n","          - The contractor may use concepts from overlapping disciplines, such as plain language, human-centered design (HCD), interaction design, and usability.\n","          - The Contractor shall explicitly support multiple models concurrently, if applicable.\n","          - The Contractor shall provide a framework for an evaluation design approach for any specific model that creates appropriate comparison groups.\n","\n","          Your extraction MUST be exact and verbatim. DO NOT summarize, paraphrase, alter, reformat, remove, or add characters or words.\n","                      \"\"\"\n","        # Enhanced prompt to capture more requirements\n","        user_prompt = f\"\"\"\n","        {system_prompt}\n","\n","          Carefully extract ALL contractor tasks explicitly and clearly assigned or implied within the following provided document text.\n","\n","          Output explicit instructions:\n","          - Provide each extracted requirement/task sentence exactly as it verbatim appears in the document text.\n","          - Output each extracted task on EXACTLY ONE (1) separate line.\n","          - Do NOT add numbering, quotation marks, symbols, bullets, or any additional characters whatsoever.\n","          - Do NOT join or split sentences; maintain each task exactly at its original sentence boundary, length, punctuation, and formatting exactly.\n","\n","          Here is the document text explicitly for your analysis and extraction:\n","          {text}\n","          \"\"\"\n","\n","        try:\n","            client = AsyncOpenAI()\n","            response = await client.chat.completions.create(\n","                model=self.model,\n","                messages=[\n","                    {\"role\": \"system\", \"content\": system_prompt},\n","                    {\"role\": \"user\", \"content\": user_prompt}\n","                ],\n","                temperature=0.7,\n","                top_p=1,\n","                frequency_penalty=0.0,\n","                presence_penalty=0.0\n","            )\n","            extracted_tasks = response.choices[0].message.content.strip().splitlines()\n","            for task in extracted_tasks:\n","                # Less restrictive filtering\n","                if len(task) > 5:\n","                    tasks.append({\n","                        #\"task_id\": task_id\n","                        \"document_id\": document_id,\n","                        \"task_text\": task,\n","                        \"task_index\": self.find_text_index(full_document_text, task)\n","                    })\n","\n","        except Exception as e:\n","            logging.error(f\"Error extracting tasks: {str(e)}\")\n","            # Fallback to regex extraction if LLM fails\n","            patterns = [\n","                r'(The\\s+contractor\\s+(?:shall|will|must)[^.]+\\.?)',  # Capture full sentence including period\n","                r'(The\\s+contractor\\s+(?:shall|will|must)[^.]+)$',    # Capture sentence without period if at end of text\n","            ]\n","\n","            for pattern in patterns:\n","                matches = re.findall(pattern, text, re.IGNORECASE)\n","                for match in matches:\n","                    tasks.append({\n","                        #\"task_id\": task_id,   # DEFINED LATER, IN THE VALIDATE_TASKS PROCEDURE\n","                        \"document_id\": document_id,\n","                        \"task_text\": match.strip(),  # Use the full match\n","                        \"task_index\": self.find_text_index(full_document_text, match)\n","                    })\n","\n","        return tasks\n"]},{"cell_type":"markdown","metadata":{"id":"sqKHz9NKXfKz"},"source":["2.3 Main Orchestration Script"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MQnzHmGia4V2"},"outputs":[],"source":["# Cell 3.1 : Main Orchestration Script\n","import asyncio\n","import os\n","import pandas as pd\n","import logging\n","from typing import List, Dict, Any\n","\n","# Database connection\n","import sqlalchemy as sa\n","from sqlalchemy import create_engine, text\n","\n","async def extract_requirements_from_rfq(pdf_path, llama_api_key, openai_api_key, db_config):\n","    \"\"\"Main function to extract requirements from an RFQ document\"\"\"\n","\n","    # Initialize processors\n","    document_processor = EnhancedDocumentProcessor(llama_api_key=llama_api_key)\n","    task_extractor = TaskExtractionAgent(api_key=openai_api_key)\n","\n","    # Reset the requirement counter for each new document\n","    task_extractor.req_counter = 1\n","\n","    # Step 1: Process document into structured nodes\n","    logging.info(f\"Processing document: {pdf_path}\")\n","    document_nodes = await document_processor.process_document(pdf_path)\n","    logging.info(f\"Generated {len(document_nodes)} document nodes\")\n","\n","    # Extract full document text\n","    full_document_text = \"\"\n","    for node in document_nodes:\n","        full_document_text += node.get(\"text\", \"\")\n","\n","    # Step 2: Extract requirements from each node\n","    all_requirements = []\n","    for node in document_nodes:\n","        # Pass full_document_text to extract_tasks\n","        node_requirements = await task_extractor.extract_tasks(node,full_document_text)\n","        for requirement in node_requirements:\n","          all_requirements.append(requirement)\n","\n","    logging.info(f\"Extracted {len(all_requirements)} requirements from document\")\n","\n","\n","    # Step 3: Validate and store requirements in database\n","    if db_config:\n","        validated_requirements = task_extractor.validate_tasks(all_requirements, \"document_id\", full_document_text)\n","        store_requirements_in_database(validated_requirements, db_config)\n","\n","    return all_requirements\n","\n","\n","def create_database_tables(engine):\n","    \"\"\"Create or update required database tables in the rfq schema\"\"\"\n","    from sqlalchemy import text\n","\n","    with engine.connect() as conn:\n","        # First ensure the 'rfq' schema exists\n","        conn.execute(text(\"CREATE SCHEMA IF NOT EXISTS rfq\"))\n","        conn.commit()\n","\n","        # Create the table with the new required columns\n","        conn.execute(text(\"\"\"\n","            CREATE TABLE IF NOT EXISTS rfq.rfq_tasks (\n","            task_id integer,\n","            document_id VARCHAR(255) NOT NULL,\n","            task_text TEXT NOT NULL,\n","            task_index integer,\n","             created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n","             )\n","        \"\"\"))\n","        conn.commit()\n","        logging.info(\"Database table schema verified/created in rfq schema\")\n","\n","def store_requirements_in_database(requirements, db_config):\n","    \"\"\"Store extracted requirements in database with proper error handling\"\"\"\n","    try:\n","        # Create connection string\n","        connection_string = f\"postgresql+psycopg://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['dbname']}\"\n","        engine = create_engine(connection_string)\n","\n","        # Ensure database schema is set up\n","        create_database_tables(engine)\n","\n","        # Create DataFrame from requirements\n","        requirements_df = pd.DataFrame(requirements)\n","        print(\"DataFrame columns:\", requirements_df.columns)\n","        print(\"requirements_df head:\",requirements_df.head())\n","\n","        # List of columns that exist in the database schema\n","        valid_columns = ['task_id', 'document_id', 'task_text', 'task_index'     ]\n","\n","        # Filter DataFrame to only include valid columns that exist in the dataframe\n","        existing_valid_columns = [col for col in valid_columns if col in requirements_df.columns]\n","        requirements_df = requirements_df[existing_valid_columns]\n","\n","        # Convert any None or NaN values to empty strings to avoid TypeErrors\n","        requirements_df = requirements_df.fillna('')\n","\n","        # Store in database - specify the rfq schema\n","        requirements_df.to_sql('rfq_tasks', engine, schema='rfq', if_exists='append', index=False)\n","        logging.info(f\"Successfully stored {len(requirements)} requirements in rfq.rfq_tasks table\")\n","\n","    except Exception as e:\n","        logging.error(f\"Error storing requirements in database: {str(e)}\")\n","        # Continue execution rather than crashing\n"]},{"cell_type":"markdown","metadata":{"id":"O8tL_m_r1Gx_"},"source":["**Step 3: Integration and Testing**\n"]},{"cell_type":"markdown","metadata":{"id":"mWtwgoxaxeDv"},"source":["3.0 (OPTIONAL) Table re-build"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P8PYVsnPYJ0S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743528423525,"user_tz":240,"elapsed":2321,"user":{"displayName":"David Scott","userId":"14102935354004324335"}},"outputId":"0557ac96-1515-4f38-8935-e91401a30316"},"outputs":[{"output_type":"stream","name":"stdout","text":["Connected to PostgreSQL database\n","Dropped existing rfq_tasks table if it existed\n","Created table: rfq.rfq_tasks with new schema\n"]}],"source":["#OPTIONAL 2_PART Table Rebuild- USE WITH CARE AND ONLY IF TRULY NECESSARY\n","# This is where the table layout can be re-built.\n","import pandas as pd\n","from sqlalchemy import create_engine, inspect\n","from urllib.parse import quote\n","from sqlalchemy.types import String, Integer\n","from sqlalchemy import text\n","\n","DB_PASSWORD=\"dG1RWVzD!F4YaneD$$\"\n","\n","import psycopg2\n","# Step 1: Verify Database Connection\n","# Database connection parameters\n","db_config = {\n","    \"host\": \"advantantus-prod.cmfo86w02i47.us-east-1.rds.amazonaws.com\",\n","    \"port\": \"5432\",\n","    \"dbname\": \"postgres\",\n","    \"user\": \"postgres\",\n","    \"password\": \"dG1RWVzD!F4YaneD$$\"\n","}\n","\n","try:\n","    # Connect to the database\n","    conn = psycopg2.connect(\n","        host=db_config['host'],\n","        port=db_config['port'],\n","        database=db_config['dbname'],\n","        user=db_config['user'],\n","        password=db_config['password']\n","    )\n","    print(\"Connected to PostgreSQL database\")\n","except Exception as e:\n","    print(f\"Error connecting to database: {e}\")\n","\n","# async def extract_requirements_from_rfq(pdf_path, llama_api_key, openai_api_key, db_config):\n","#    \"\"\"Main function to extract requirements from an RFQ document\"\"\"\n","\n","\n","#Step 2: Drop (if exists)  and Create the table\n","\n","# Create Database Connection\n","encoded_password = quote(DB_PASSWORD, safe='')\n","connection_url = (\n","    f\"postgresql+psycopg2://postgres:{encoded_password}@\"\n","    \"advantantus-prod.cmfo86w02i47.us-east-1.rds.amazonaws.com:5432/postgres\"\n",")\n","engine = create_engine(connection_url)\n","\n","with engine.begin() as conn:\n","    # This creates a transaction that will be committed at the end of the block\n","    conn.execute(text('DROP TABLE IF EXISTS \"rfq\".\"rfq_tasks\" CASCADE;'))\n","    print(\"Dropped existing rfq_tasks table if it existed\")\n","\n","# Create the table with the new schema\n","with engine.begin() as conn:  # Using begin() to ensure transaction is committed\n","    # Verify table doesn't exist before creating\n","    result = conn.execute(text(\"\"\"\n","        SELECT EXISTS (\n","            SELECT 1 FROM information_schema.tables\n","            WHERE table_schema = 'rfq' AND table_name = 'rfq_tasks'\n","        );\n","    \"\"\"))\n","\n","    if not result.scalar():\n","      # Create the new table with option to updated schema\n","        conn.execute(text(\"\"\"\n","        CREATE TABLE \"rfq\".\"rfq_tasks\" (\n","            task_id integer,\n","            document_id VARCHAR(255) NOT NULL,\n","            task_text TEXT NOT NULL,\n","            task_index integer\n","            );\n","        \"\"\"))\n","        print(\"Created table: rfq.rfq_tasks with new schema\")\n","    else:\n","      print(\"Table rfq_tasks already exists - verify schema manually\")"]},{"cell_type":"markdown","metadata":{"id":"mx8mHyhkPxhT"},"source":["**SECTION 3.1: RUN-TIME SCRIPT**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nFQ-p6uHUlcl","outputId":"e5d3edca-d864-4e89-82d3-6193819d1396","executionInfo":{"status":"ok","timestamp":1743528622857,"user_tz":240,"elapsed":177532,"user":{"displayName":"David Scott","userId":"14102935354004324335"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Started parsing the file under job_id bb2c0d24-6767-41d2-aaae-24d34d1b4cf2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:Task text not found in document: The Contractor shall develop, implement, monitor, analyze, and provide technical assistance for innovative care delivery and payment models. \n","WARNING:root:Task text not found in document: The Contractor shall build collaborative learning networks to disseminate best practices. \n","WARNING:root:Task text not found in document: The Contractor shall develop the necessary technology to support this activity. \n","WARNING:root:Task text not found in document: The Contractor shall conduct rapid cycle analysis of the programs and participants.  \n","WARNING:root:Task text not found in document: The Contractor shall provide rapid reporting of interim findings and summative findings as feasible.\n","WARNING:root:Task text not found in document: The Contractor shall advance CMS priorities, including but not limited to patient/beneficiary/caregiver engagement and experience, and multi-payer alignment.\n","WARNING:root:Task text not found in document: The Contractor shall provide data-driven support for targeted model participant recruitment and model launch activities.\n","WARNING:root:Task text not found in document: The Contractor shall manage and coordinate learning systems for CMMI models, including:\n","WARNING:root:Task text not found in document: I'm sorry, but it seems there is no task-related content provided from the document text for analysis and extraction. Could you please provide the actual text from the document where contractor tasks are detailed?\n","WARNING:root:Task text not found in document: The Contractor shall identify feedback from participants about how CMS data feedback is being used and how it can be improved.  \n","WARNING:root:Task text not found in document: The Contractor shall conduct site visits with structured qualitative interviews to capture ongoing learning and emerging strategies and tactics being implemented by model participants.  \n","WARNING:root:Task text not found in document: The Contractor shall provide support for peer-to-peer learning from site visits to organizations with early indications of success.  \n","WARNING:root:Task text not found in document: The Contractor shall develop case studies, spotlight articles, and reports to highlight emerging strategies and tactics.  \n","WARNING:root:Task text not found in document: The Contractor shall identify high-performing practices within models, discover and analyze tactical and strategic patterns of best practices in care delivery, unearth contextual enablers of innovation, and diffuse best practices nationally.  \n","WARNING:root:Task text not found in document: The Contractor shall stratify practices based on performance in the model and relative to health outcomes to ensure year-over-year improvements in model goals.  \n","WARNING:root:Task text not found in document: The Contractor shall monitor and report the participation of organizations and participants in learning activities.  \n","WARNING:root:Task text not found in document: The Contractor shall develop and maintain meaningful views (e.g., maps and graphs) demonstrating engagement of organizations in CMMI models, the spread of specific activities, and emerging results demonstrating progress in achieving the three measures of better health, better care, and lower costs through improvement for each model.  \n","WARNING:root:Task text not found in document: The Contractor shall develop and maintain a knowledge management system for each model, including documentation structure and organization in a web portal environment and a systematic method to capture lessons learned.  \n","WARNING:root:Task text not found in document: The Contractor shall use a web-based platform that CMS shall provide for knowledge sharing, collaboration, networking, and reporting with the capacity to support multiple, parallel learning communities involved in learning and diffusion activities of the CMS.  \n","WARNING:root:Task text not found in document: The Contractor shall use community and network management principles to foster collaboration among community participants.  \n","WARNING:root:Task text not found in document: The Contractor shall use data to understand and improve how model participants engage with the web-based platform.  \n","WARNING:root:Task text not found in document: The Contractor shall identify functionality that can improve the platform based on industry best practices, monitoring of use, and user input.  \n","WARNING:root:Task text not found in document: The Contractor shall collaborate with CMS to provide the requirements to the IT Contractor that maintains the web-based platform.  \n","WARNING:root:Task text not found in document: The Contractor shall support models using the web platform by monitoring site activity and helping staff and participants make optimal use of the platform’s functionality.  \n","WARNING:root:Task text not found in document: The Contractor shall develop business relationships capable of providing improvement and transformation support to organizations participating in program models and fostering engagement and collaboration among model participants.\n","WARNING:root:Task text not found in document: I'm sorry, but it seems the document text intended for analysis and extraction is missing. Please provide the necessary text so I can assist you accordingly.\n"]},{"output_type":"stream","name":"stdout","text":["DataFrame columns: Index(['task_id', 'document_id', 'task_text', 'task_index'], dtype='object')\n","rfq tasks inserted successfully.\n","Successfully stored 286 requirements in rfq.rfq_tasks table\n","               document_id                                          task_text  \\\n","0  75FCMC25RJ018_Section_C  The Contractor shall develop, implement, monit...   \n","1  75FCMC25RJ018_Section_C  The Contractor shall build collaborative learn...   \n","2  75FCMC25RJ018_Section_C  The Contractor shall develop the necessary tec...   \n","3  75FCMC25RJ018_Section_C  The Contractor shall conduct rapid cycle analy...   \n","4  75FCMC25RJ018_Section_C  The Contractor shall provide rapid reporting o...   \n","5  75FCMC25RJ018_Section_C  The Contractor shall work with CMS on matters ...   \n","6  75FCMC25RJ018_Section_C  - Supporting all aspects of model design and o...   \n","7  75FCMC25RJ018_Section_C  - Conducting program, data, and environmental ...   \n","8  75FCMC25RJ018_Section_C           - Monitoring model site implementations.   \n","9  75FCMC25RJ018_Section_C  - Designing and carrying out surveys and other...   \n","\n","   task_index  \n","0          -1  \n","1          -1  \n","2          -1  \n","3          -1  \n","4          -1  \n","5        2851  \n","6        2927  \n","7        3016  \n","8        3072  \n","9        3113  \n","Successfully exported 290 requirements to rfq_requirements_export.xlsx\n","Successfully exported 26 Non-indexed requirements to rfq_requirements_export_NOINDEX.xlsx\n"]}],"source":["#Updated v. of Run-time script (from Copy of Advantantus_2):\n","#Run-time  Cell: Run the extraction with your RFQ\n","import logging\n","import asyncio\n","import pandas as pd\n","from sqlalchemy import create_engine, text\n","import nest_asyncio\n","\n","# Enable nested asyncio (needed for Jupyter)\n","nest_asyncio.apply()\n","\n","#OPTIONAL DEFINITION OF OOPENAI api KEY:\n","openai_api_key=\"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\",\n","\n","\n","# Set up logging\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","\n","def create_database_tables(engine):\n","    \"\"\"Create or update required database tables in the rfq schema\"\"\"\n","    from sqlalchemy import text\n","\n","    with engine.connect() as conn:\n","        # First ensure the 'rfq' schema exists\n","        conn.execute(text(\"CREATE SCHEMA IF NOT EXISTS rfq\"))\n","        conn.commit()\n","\n","        # Create the table with the new required columns\n","        conn.execute(text(\"\"\"\n","            CREATE TABLE IF NOT EXISTS \"rfq\".\"rfq_tasks\" (\n","                task_id integer,\n","                document_id VARCHAR(255) NOT NULL,\n","                task_text TEXT NOT NULL,\n","                task_index numeric\n","            )\n","        \"\"\"))\n","        conn.commit()\n","        logging.info(\"Database tables verified/created in rfq schema\")\n","\n","\n","def store_requirements_in_database(requirements, db_config):\n","    \"\"\"Store extracted requirements in database with proper error handling\"\"\"\n","    try:\n","        # Create connection string\n","        connection_string = f\"postgresql+psycopg://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['dbname']}\"\n","        engine = create_engine(connection_string)\n","\n","        # Ensure database schema is set up\n","        create_database_tables(engine)\n","\n","        # Create DataFrame from requirements\n","        requirements_df = pd.DataFrame(requirements)\n","        print(\"DataFrame columns:\", requirements_df.columns)\n","\n","        # Drop rows with task_index < 0\n","        requirements_df = requirements_df[requirements_df['task_index'] >= 0]\n","\n","        # List of columns that exist in the database schema\n","        valid_columns = ['task_id', 'document_id', 'task_text', 'task_index' ]\n","\n","        # Filter DataFrame to only include valid columns that exist in the dataframe\n","        existing_valid_columns = [col for col in valid_columns if col in requirements_df.columns]\n","        requirements_df = requirements_df[existing_valid_columns]\n","\n","        # Convert any None or NaN values to empty strings to avoid TypeErrors\n","        requirements_df = requirements_df.fillna('')\n","\n","        # Store in database - specify the rfq schema\n","        #requirements_df.to_sql('rfq_tasks', engine, schema='rfq', if_exists='append', index=False)\n","        requirements_df.to_sql( name='rfq_tasks',con=engine, schema='rfq', if_exists='append',index=False)\n","        print(\"rfq tasks inserted successfully.\")\n","\n","        logging.info(f\"Successfully stored {len(requirements_df)} requirements in rfq.rfq_tasks table\")\n","        print(f\"Successfully stored {len(requirements_df)} requirements in rfq.rfq_tasks table\")\n","    except Exception as e:\n","        logging.error(f\"Error storing requirements in database: {str(e)}\")\n","        # Continue execution rather than crashing\n","\n","\n","async def extract_requirements_from_rfq(pdf_path, llama_api_key, openai_api_key, db_config):\n","    \"\"\"Main function to extract requirements from an RFQ document\"\"\"\n","\n","    # Initialize processors\n","    document_processor = EnhancedDocumentProcessor(llama_api_key=llama_api_key)\n","    task_extractor = TaskExtractionAgent(api_key=openai_api_key)\n","\n","    # Reset the requirement counter for each new document\n","    task_extractor.req_counter = 1\n","\n","    # Step 1: Process document into structured nodes\n","    logging.info(f\"Processing document: {pdf_path}\")\n","    document_nodes = await document_processor.process_document(pdf_path)\n","    logging.info(f\"Generated {len(document_nodes)} document nodes\")\n","    # Extract full document text\n","    full_document_text = \"\"\n","    for node in document_nodes:\n","        full_document_text += node.get(\"text\", \"\")\n","\n","    # Step 2: Extract requirements from each node\n","    all_requirements = []\n","    for node in document_nodes:\n","        node_requirements = await task_extractor.extract_tasks(node,full_document_text)\n","        all_requirements.extend(node_requirements)\n","\n","    logging.info(f\"Extracted {len(all_requirements)} requirements from document\")\n","    # Step 3: Store requirements in database\n","    if db_config:\n","      validated_requirements = task_extractor.validate_tasks(all_requirements, \"document_id\", full_document_text)\n","      store_requirements_in_database(validated_requirements, db_config)\n","    # PRIOR\n","    #Step 3: Store requirements in database\n","    #if db_config:\n","    #    store_requirements_in_database(all_requirements, db_config)\n","\n","    return all_requirements\n","\n","# Main execution\n","async def main():\n","    # Configuration\n","    config = {\n","        \"llama_api_key\": \"llx-mW09cH1F4xP5F9EKHHVNptBhOkbXMjPu1zr1Xzek7Zt2kuZa\",\n","        \"openai_api_key\": \"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\",\n","        \"pdf_path\":\"75FCMC25RJ018_Section_C.pdf\",\n","                      #\"Attachment 1 - PALS SOW.pdf\",#\"COREQ_SOW.pdf\", #\"PM3 II Consolidated PWS 02132024.pdf\",\n","                      #  ,\"SA InsightsAI II 06172022 FE.pdf\",'RMADA_II_75FCMC25RJ018_Section_C.pdf', \"75FCMC25RJ018_ALL_SECTIONS.pdf\",\n","        \"db_config\": {\n","            \"host\": \"advantantus-prod.cmfo86w02i47.us-east-1.rds.amazonaws.com\",\n","            \"port\": \"5432\",\n","            \"dbname\": \"postgres\",\n","            \"user\": \"postgres\",\n","            \"password\": \"dG1RWVzD!F4YaneD$$\"\n","        }\n","    }\n","\n","    # Check if PDF exists\n","    try:\n","        with open(config[\"pdf_path\"], 'rb') as f:\n","            pdf_exists = True\n","        logging.info(f\"PDF file found at {config['pdf_path']}\")\n","    except FileNotFoundError:\n","        pdf_exists = False\n","        logging.error(f\"PDF file not found at {config['pdf_path']}. Please upload it.\")\n","        return\n","\n","    # Process the document\n","    if pdf_exists:\n","        requirements = await extract_requirements_from_rfq(\n","            config[\"pdf_path\"],\n","            config[\"llama_api_key\"],\n","            config[\"openai_api_key\"],\n","            config[\"db_config\"]\n","        )\n","\n","        # Display results with new column structure\n","        logging.info(f\"Extracted {len(requirements)} requirements (includes non-indexed rows to be dropped later)\")\n","        df = pd.DataFrame(requirements)\n","        print(df.head(10))\n","\n","        # Export to Excel/CSV with new column structure\n","        def export_requirements_to_excel(req_df, filename=\"rfq_requirements_export.xlsx\", filename2=\"rfq_requirements_export_NOINDEX.xlsx\"):\n","          \"\"\"Export requirements to Excel spreadsheet\"\"\"\n","          try:\n","              # Drop rows with task_index < 0\n","              req_df_indexed = req_df[req_df['task_index'] >= 0]\n","              req_df_no_index = req_df[req_df['task_index'] < 0]\n","\n","              # Install openpyxl if needed\n","              try:\n","                  import openpyxl\n","              except ImportError:\n","                  import pip\n","                  pip.main(['install', 'openpyxl'])\n","\n","              # Export to Excel\n","              req_df_indexed.to_excel(filename, index=False, engine='openpyxl')\n","              print(f\"Successfully exported {len(req_df_indexed)} requirements to {filename}\")\n","\n","              if not req_df_no_index.empty:\n","                  req_df_no_index.to_excel(filename2, index=False, engine='openpyxl')\n","                  print(f\"Successfully exported {len(req_df_no_index)} Non-indexed requirements to {filename2}\")\n","              else:\n","                  print(f\"No non-indexed requirements found.\")\n","\n","          except Exception as e:\n","              print(f\"Error exporting to Excel: {e}\")\n","              # Fallback to CSV if Excel export fails\n","              csv_filename = filename.replace('.xlsx', '.csv')\n","              req_df_indexed.to_csv(csv_filename, index=False)\n","              print(f\"Exported to CSV instead: {csv_filename}\")\n","\n","              if not req_df_no_index.empty:\n","                  csv_filename2 = filename2.replace('.xlsx', '.csv')\n","                  req_df_no_index.to_csv(csv_filename2, index=False)\n","                  print(f\"Exported to CSV instead: {csv_filename2}\")\n","              else:\n","                  print(f\"No non-indexed requirements found.\")\n","\n","        # Call the function to export requirements\n","        if requirements:\n","            df = pd.DataFrame(requirements)\n","            export_requirements_to_excel(df)\n","\n","        return requirements\n","\n","# Run the script\n","if __name__ == \"__main__\":\n","    asyncio.run(main())\n","else:\n","    # For Jupyter notebook\n","    requirements = await main()\n"]},{"cell_type":"markdown","metadata":{"id":"oWjnTbE81Z0n"},"source":["Section 3.2: Validation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1477,"status":"ok","timestamp":1743528656382,"user":{"displayName":"David Scott","userId":"14102935354004324335"},"user_tz":240},"id":"3WNK29i6Mr9J","outputId":"3ad9ecac-5cf5-47b5-f121-68fcf86332ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["Validating `rfq.rfq_tasks`...\n","✅ `rfq.rfq_tasks` table contains 286 rows.\n","     task_id  document_id                                          task_text  \\\n","0          1  document_id  The Contractor shall work with CMS on matters ...   \n","1          2  document_id  - Supporting all aspects of model design and o...   \n","2          3  document_id  - Conducting program, data, and environmental ...   \n","3          4  document_id           - Monitoring model site implementations.   \n","4          5  document_id  - Designing and carrying out surveys and other...   \n","..       ...          ...                                                ...   \n","281      282  document_id  The Contractor shall provide CMS with a copy o...   \n","282      283  document_id  All CDs, DVDs, data tapes, and other files sha...   \n","283      284  document_id  Analytic files must be accompanied by appropri...   \n","284      285  document_id  When required, the Contractor shall provide su...   \n","285      286  document_id  The Contractor shall produce both a high level...   \n","\n","     task_index  \n","0          2851  \n","1          2927  \n","2          3016  \n","3          3072  \n","4          3113  \n","..          ...  \n","281       83700  \n","282       83800  \n","283       83911  \n","284       84011  \n","285       84336  \n","\n","[286 rows x 4 columns]\n"]}],"source":["# 3.4 Validate rfq.rfq_tasks Table\n","import pandas as pd\n","from sqlalchemy import create_engine, inspect\n","from urllib.parse import quote\n","#from sqlalchemy.types import String, Integer\n","#from sqlalchemy import text\n","\n","DB_PASSWORD=\"dG1RWVzD!F4YaneD$$\"\n","# Create Database Connection\n","encoded_password = quote(DB_PASSWORD, safe='')\n","connection_url = (\n","    f\"postgresql+psycopg2://postgres:{encoded_password}@\"\n","    \"advantantus-prod.cmfo86w02i47.us-east-1.rds.amazonaws.com:5432/postgres\"\n",")\n","engine = create_engine(connection_url)\n","\n","print(\"Validating `rfq.rfq_tasks`...\")\n","\n","rfq_tasks_df = pd.read_sql('SELECT * FROM \"rfq\".\"rfq_tasks\"', con=engine)\n","if rfq_tasks_df.empty:\n","    print(\"❌ `rfq.rfq_tasks` table is empty.\")\n","else:\n","    print(f\"✅ `rfq.rfq_tasks` table contains {len(rfq_tasks_df)} rows.\")\n","    print(rfq_tasks_df.tail(405))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"elapsed":57,"status":"ok","timestamp":1743426666837,"user":{"displayName":"David Scott","userId":"14102935354004324335"},"user_tz":240},"id":"shCVVm3IUH_F","outputId":"4239e08c-3006-47db-e3cf-d459ac847565"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["     task_id  document_id                                          task_text  \\\n","0          1  document_id  The Contractor shall work with CMS on matters ...   \n","1          2  document_id  - Supporting all aspects of model design and o...   \n","2          3  document_id  - Conducting program, data, and environmental ...   \n","3          4  document_id         - Monitoring model site implementations.     \n","4          5  document_id  - Designing and carrying out surveys and other...   \n","..       ...          ...                                                ...   \n","332      330  document_id  The requestor or custodian listed on the DUA s...   \n","333      331  document_id  Only one closure request can be sent for one D...   \n","221      332  document_id  The requestor or custodian shall fill out the ...   \n","187      333  document_id  This form must be printed, signed, scanned, an...   \n","257      334  document_id  In the email, the Subject line should read as ...   \n","\n","     task_index  \n","0          2851  \n","1          2927  \n","2          3016  \n","3          3072  \n","4          3113  \n","..          ...  \n","332       84517  \n","333       84749  \n","221       84862  \n","187       85016  \n","257       85210  \n","\n","[334 rows x 4 columns]"],"text/html":["\n","  <div id=\"df-276a6322-638a-4b02-97a5-0749ac21296f\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>task_id</th>\n","      <th>document_id</th>\n","      <th>task_text</th>\n","      <th>task_index</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>document_id</td>\n","      <td>The Contractor shall work with CMS on matters ...</td>\n","      <td>2851</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>document_id</td>\n","      <td>- Supporting all aspects of model design and o...</td>\n","      <td>2927</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>document_id</td>\n","      <td>- Conducting program, data, and environmental ...</td>\n","      <td>3016</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>document_id</td>\n","      <td>- Monitoring model site implementations.</td>\n","      <td>3072</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>document_id</td>\n","      <td>- Designing and carrying out surveys and other...</td>\n","      <td>3113</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>332</th>\n","      <td>330</td>\n","      <td>document_id</td>\n","      <td>The requestor or custodian listed on the DUA s...</td>\n","      <td>84517</td>\n","    </tr>\n","    <tr>\n","      <th>333</th>\n","      <td>331</td>\n","      <td>document_id</td>\n","      <td>Only one closure request can be sent for one D...</td>\n","      <td>84749</td>\n","    </tr>\n","    <tr>\n","      <th>221</th>\n","      <td>332</td>\n","      <td>document_id</td>\n","      <td>The requestor or custodian shall fill out the ...</td>\n","      <td>84862</td>\n","    </tr>\n","    <tr>\n","      <th>187</th>\n","      <td>333</td>\n","      <td>document_id</td>\n","      <td>This form must be printed, signed, scanned, an...</td>\n","      <td>85016</td>\n","    </tr>\n","    <tr>\n","      <th>257</th>\n","      <td>334</td>\n","      <td>document_id</td>\n","      <td>In the email, the Subject line should read as ...</td>\n","      <td>85210</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>334 rows × 4 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-276a6322-638a-4b02-97a5-0749ac21296f')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-276a6322-638a-4b02-97a5-0749ac21296f button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-276a6322-638a-4b02-97a5-0749ac21296f');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-24f0c9e4-2ec7-42ac-9db5-319d228682c3\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-24f0c9e4-2ec7-42ac-9db5-319d228682c3')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-24f0c9e4-2ec7-42ac-9db5-319d228682c3 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"rfq_tasks_df\",\n  \"rows\": 334,\n  \"fields\": [\n    {\n      \"column\": \"task_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 96,\n        \"min\": 1,\n        \"max\": 334,\n        \"num_unique_values\": 334,\n        \"samples\": [\n          26,\n          310,\n          74\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"document_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"document_id\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"task_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 334,\n        \"samples\": [\n          \"The Contractor shall only develop the proposed system if CMS approves.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"task_index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 24825,\n        \"min\": 2851,\n        \"max\": 85210,\n        \"num_unique_values\": 331,\n        \"samples\": [\n          6389\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":8}],"source":["#View Task DataFrame\n","rfq_tasks_df.sort_values(by=['task_index'], ascending=True) # Assuming you want to sort by the 'task_index' column\n"]},{"cell_type":"markdown","metadata":{"id":"Gxk4Ji7Q5is9"},"source":["################################################################################################################################################################################################################################################\n","\n","**END OF PROGRAM**\n","\n","################################################################################################################################################################################################################################################\n"]},{"cell_type":"markdown","metadata":{"id":"Gjx_Mx6mWK7k"},"source":["** (incomplete) 3.3 Post-Production Unit Tests:**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":110},"executionInfo":{"elapsed":9,"status":"error","timestamp":1741112167678,"user":{"displayName":"David Scott","userId":"14102935354004324335"},"user_tz":300},"id":"NBE7Mp7mWPAt","outputId":"712a7d96-2f1f-455f-b259-b45f6cee020a"},"outputs":[{"ename":"SyntaxError","evalue":"invalid decimal literal (<ipython-input-48-768689b325bf>, line 11)","output_type":"error","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-48-768689b325bf>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    from ipython-input-30-409f25fb0845 import EnhancedDocumentProcessor  # This is where it's defined\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"]}],"source":["# Unit Tests for RFQ Requirement Extraction\n","import asyncio\n","import os\n","import pandas as pd\n","import logging\n","import pytest\n","from typing import List, Dict, Any\n","# Database connection\n","import sqlalchemy as sa\n","from sqlalchemy import create_engine, text\n","\n","\n","# Skip tests if API keys not available\n","pytestmark = pytest.mark.skipif(\n","    not os.environ.get(\"LLAMA_CLOUD_API_KEY\") or\n","    not os.environ.get(\"OPENAI_API_KEY\"),\n","    reason=\"API keys not available\"\n",")\n","\n","@pytest.fixture\n","def sample_pdf_path():\n","    return \"75FCMC25RJ018_ALL_SECTIONS.pdf\"\n","\n","@pytest.fixture\n","def document_processor():\n","    return EnhancedDocumentProcessor(\n","        llama_api_key=os.environ.get(\"LLAMA_CLOUD_API_KEY\")\n","    )\n","\n","@pytest.fixture\n","def task_extractor():\n","    return TaskExtractionAgent(\n","        api_key=os.environ.get(\"OPENAI_API_KEY\")\n","    )\n","\n","@pytest.mark.asyncio\n","async def test_document_processing(document_processor, sample_pdf_path):\n","    \"\"\"Test that document processing extracts structured nodes with page numbers and headers\"\"\"\n","    nodes = await document_processor.process_document(sample_pdf_path)\n","\n","    # Verify we got nodes\n","    assert len(nodes) > 0\n","\n","    # Verify node structure with new required fields\n","    for node in nodes:\n","        assert \"text\" in node\n","        assert \"metadata\" in node\n","        assert \"section_path\" in node[\"metadata\"]\n","        assert \"section_level\" in node[\"metadata\"]\n","        assert \"page_number\" in node[\"metadata\"]\n","        assert \"header\" in node[\"metadata\"]\n","        assert \"document_id\" in node[\"metadata\"]\n","\n","def test_requirement_extraction(task_extractor):\n","    \"\"\"Test that requirement extraction produces structured requirements with new schema\"\"\"\n","    # Sample node with \"shall\" statements\n","    sample_node = {\n","        \"text\": \"The Contractor shall validate payment calculations using SAS code.\",\n","        \"metadata\": {\n","            \"section_path\": \"C.3.ii.10 Payment Validation\",\n","            \"section_level\": 2,\n","            \"document_id\": \"sample_doc\",\n","            \"page_number\": 42,\n","            \"header\": \"C.3.ii.10 Payment Validation\"\n","        }\n","    }\n","\n","    # Reset counter to ensure predictable task_id\n","    task_extractor.req_counter = 1\n","\n","    requirements = task_extractor.extract_tasks(sample_node)\n","\n","    # Verify requirements were extracted\n","    assert len(requirements) > 0\n","\n","    # Verify requirement structure with new fields\n","    requirement = requirements[0]\n","    assert \"document_id\" in requirement\n","    assert \"page_number\" in requirement\n","    assert \"task_id\" in requirement\n","    assert \"header\" in requirement\n","    assert \"req\" in requirement\n","    assert \"task_text\" in requirement\n","\n","    # Verify content matches expected format\n","    assert \"validate payment\" in requirement[\"task_text\"].lower()\n","    assert requirement[\"task_id\"] == \"001\"  # First requirement should be 001\n","    assert requirement[\"page_number\"] == 42\n","    assert requirement[\"header\"] == \"C.3.ii.10 Payment Validation\"\n","\n","    # Verify req contains a technical skill area name\n","    assert len(requirement[\"req\"]) > 0\n","    assert isinstance(requirement[\"req\"], str)\n"]},{"cell_type":"markdown","metadata":{"id":"kut59FeS5kBf"},"source":["** END Program**\n"]},{"cell_type":"markdown","metadata":{"id":"ZxP2jdUwUGop"},"source":["** Appendix:**\n","\n","**Task Extraction: Chain-of-Thought Example**\n","\n","Here's how the chain-of-thought extraction works on a real RFQ section:\n","\n","*Input Text:*\n","\n","text\n","**C.3.ii.10 Payment Validation\n","The Contractor shall validate payment calculations using SAS code. The validation must compare results against CMS internal calculations. Discrepancy reports should be produced weekly, listing any variations greater than 0.1%.**\n","\n","\n","*Chain-of-Thought Processing:*\n","\n","Analysis:\n","\n","Explicit requirement: \"The Contractor shall validate payment calculations using SAS code\"\n","\n","Explicit requirement: \"The validation must compare results against CMS internal calculations\"\n","\n","Implicit requirement: Production of weekly discrepancy reports\n","\n","*Categorization:*\n","\n","Type: PROCESS (validation task) and REPORTING (discrepancy reports)\n","\n","Criticality: HIGH (uses \"shall\" and \"must\")\n","\n","Dependencies: Requires access to CMS internal calculations\n","\n","*Extraction:*\n","\n","Creates structured JSON for each requirement\n","\n","Preserves context from section C.3.ii.10\n","\n","Includes timeframe (weekly) for reporting requirement\n","\n","*Output:*\n","\n","json\n","[\n","  {\n","    \"task_id\": \"C3ii10_1\",\n","    \"task_text\": \"The Contractor shall validate payment calculations using SAS code\",\n","    \"task_type\": \"PROCESS\",\n","    \"criticality\": \"HIGH\",\n","    \"implied\": false,\n","    \"dependencies\": [\"Access to CMS internal calculations\"],\n","    \"timeframe\": null,\n","    \"source_section\": \"C.3.ii.10 Payment Validation\",\n","    \"context\": \"Payment validation against CMS internal calculations\"\n","  },\n","  {\n","    \"task_id\": \"C3ii10_2\",\n","    \"task_text\": \"The validation must compare results against CMS internal calculations\",\n","    \"task_type\": \"PROCESS\",\n","    \"criticality\": \"HIGH\",\n","    \"implied\": false,\n","    \"dependencies\": [],\n","    \"timeframe\": null,\n","    \"source_section\": \"C.3.ii.10 Payment Validation\",\n","    \"context\": \"Part of payment validation process\"\n","  },\n","  {\n","    \"task_id\": \"C3ii10_3\",\n","    \"task_text\": \"Produce weekly discrepancy reports listing variations greater than 0.1%\",\n","    \"task_type\": \"REPORTING\",\n","    \"criticality\": \"MEDIUM\",\n","    \"implied\": true,\n","    \"dependencies\": [\"C3ii10_1\", \"C3ii10_2\"],\n","    \"timeframe\": \"weekly\",\n","    \"source_section\": \"C.3.ii.10 Payment Validation\",\n","    \"context\": \"Output of validation process\"\n","  }\n","]"]}],"metadata":{"colab":{"provenance":[{"file_id":"1fdgCEB2-vpT8zUhnigBXA7_maG27_cJi","timestamp":1741642936789},{"file_id":"1aSfkxn-qll6jSmKGnm0ZYwxa-0m_Z5YC","timestamp":1736965884720},{"file_id":"1MTEes5Pg1gG9vFWNLZHQSkvVCZSJ18fJ","timestamp":1721921977471},{"file_id":"1Nt3oUy5wzTxxx7tnAPPkdYPbiOY-GWd-","timestamp":1719952770966},{"file_id":"1pe5jBrugYlJZg_dka48Ap-uAzejTzPcl","timestamp":1719836665060},{"file_id":"1NMmbgrnZWuu6Zfo2rh7T_ElqfR9ZT4Sg","timestamp":1711048919448},{"file_id":"1xoGd_VXQHl0krbMvYcPInHWmUn4hSK7y","timestamp":1709671785742},{"file_id":"1ic00fCnFiptd_GEnThHbTeaanJItqnoC","timestamp":1708449146973}],"authorship_tag":"ABX9TyN9mmKm3O6nr7vL9c+VY/UP"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}